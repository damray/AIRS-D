version: '3.8'

services:
  # Backend Dev (Node.js avec hot reload)
  backend-dev:
    image: node:20-alpine
    container_name: airs-chatbot-backend-dev
    working_dir: /app
    ports:
      - "3001:3001"
    volumes:
      - ./server:/app
      - /app/node_modules
    environment:
      # Backend Configuration
      - BACKEND_PORT=3001
      - FRONTEND_URL=http://localhost:5173

      # AIRS Configuration (secrets cachés)
      - AIRS_API_URL=${AIRS_API_URL:-https://service.api.aisecurity.paloaltonetworks.com}
      - AIRS_API_TOKEN=${AIRS_API_TOKEN}
      - AIRS_PROFILE_NAME=${AIRS_PROFILE_NAME}

      # Google Vertex AI Configuration
      - VERTEX_PROJECT_ID=${VERTEX_PROJECT_ID}
      - VERTEX_LOCATION=${VERTEX_LOCATION:-us-central1}
      - VERTEX_API_KEY=${VERTEX_API_KEY}

      # Anthropic Configuration
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY}

      # Ollama Configuration
      - OLLAMA_API_URL=${OLLAMA_API_URL:-http://ollama:11434/api/chat}

      # Azure OpenAI Configuration
      - AZURE_OPENAI_ENDPOINT=${AZURE_OPENAI_ENDPOINT}
      - AZURE_OPENAI_API_KEY=${AZURE_OPENAI_API_KEY}
      - AZURE_OPENAI_DEPLOYMENT=${AZURE_OPENAI_DEPLOYMENT:-gpt-4}
    command: sh -c "npm install && npm run dev"
    networks:
      - airs-network
    restart: unless-stopped
    depends_on:
      - ollama

  # Frontend Dev (Vite avec hot reload)
  frontend-dev:
    image: node:20-alpine
    container_name: airs-chatbot-frontend-dev
    working_dir: /app
    ports:
      - "5173:5173"
    volumes:
      - .:/app
      - /app/node_modules
    environment:
      # Rendez le watcher plus robuste dans Docker (Mac/Windows)
      - CHOKIDAR_USEPOLLING=true
      - WATCHPACK_POLLING=true

      # Backend URL
      - VITE_BACKEND_URL=http://localhost:3001

      # Supabase Configuration
      - VITE_SUPABASE_URL=${VITE_SUPABASE_URL}
      - VITE_SUPABASE_ANON_KEY=${VITE_SUPABASE_ANON_KEY}
    command: sh -c "npm install && npm run dev -- --host 0.0.0.0 --port 5173"
    env_file:
      - .env
    networks:
      - airs-network
    restart: unless-stopped
    depends_on:
      - backend-dev

  # Ollama (Local LLM server)
  ollama:
    image: ollama/ollama:latest
    container_name: airs-ollama-dev
    ports:
      - "11434:11434"
    volumes:
      - ollama-data:/root/.ollama   # persist models
    networks:
      - airs-network
    restart: unless-stopped
    profiles:
      - with-ollama
    healthcheck:
      test: ["CMD", "curl", "-fsS", "http://localhost:11434/api/tags"]
      interval: 10s
      timeout: 3s
      retries: 20
      start_period: 10s
    # GPU (optionnel, si NVIDIA runtime dispo)
    # gpus: all

  # One-shot job to pull models if missing
  ollama-init:
    image: ollama/ollama:latest
    container_name: airs-ollama-init
    depends_on:
      ollama:
        condition: service_healthy
    environment:
      - OLLAMA_HOST=http://ollama:11434

      - OLLAMA_MODELS=llama3.1:8b mistral:7b-instruct
    volumes:
      - ollama-data:/root/.ollama
    networks:
      - airs-network
    restart: "no"
    entrypoint: ["/bin/sh","-lc"]
    command: |
      set -e
      echo "Models to ensure present: $OLLAMA_MODELS"
      for m in $OLLAMA_MODELS; do
        if ollama list | awk '{print $1":"$2}' | grep -q "^${m}$"; then
          echo "✔ $m already present, skipping pull"
        else
          echo "↓ Pulling $m ..."
          ollama pull "$m"
        fi
      done
      echo "All requested models are available."
    profiles:
      - with-ollama

networks:
  airs-network:
    driver: bridge

volumes:
  ollama-data:
    driver: local